---
layout: single

title: "[Linux] Threads for advanced performance"
excerpt: "Makefile: dependency rule of files"

categories:
  - Linux server

tag: [C/C++, linux, thread] 

toc: true
toc_sticky: true

author_profile: false
sidebar:
  nav: "docs"
---

![header](https://capsule-render.vercel.app/api?type=rect&color=20:660099,100:E2231A)


# POSIX Thread

**Advantages:**

- Parallelism: Threads enable concurrent task execution, harnessing the power of multi-core processors for improved performance.

- Responsiveness: By leveraging threads, applications can maintain a responsive user interface, ensuring a smooth user experience even during resource-intensive operations.

- Resource Sharing: Threads within a process share resources, streamlining communication and data exchange among different parts of the program.

- Simplicity of Communication: Shared memory space simplifies communication between threads, facilitating efficient data exchange.

**Disadvantages:**

- Complexity: Multi-threaded programming introduces complexity, posing challenges in understanding and managing the order of execution.

- Race Conditions: Shared data among threads can lead to race conditions, introducing unpredictability and potential issues.

- Deadlocks: Threads may deadlock, causing a program freeze as threads wait indefinitely for resources held by others.

- Overhead: Creating and managing threads incurs overhead, impacting system resources and performance.

- Difficulty in Debugging: Debugging threaded programs is challenging due to non-deterministic behavior, making issue reproduction and diagnosis more difficult.

In conclusion, while threads offer advantages in parallelism and responsiveness, their usage introduces complexities and potential pitfalls, necessitating careful consideration in the context of specific application requirements and constraints.

## threads programm

>**#include &lt;pthread.h&gt;**

pthread.h header file, which is necessary when working with POSIX threads in C. This header file provides the declarations and definitions needed for thread-related functions and types.

>**int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg);**

Parameters:<br><br>
**thread**: A pointer to a pthread_t object where the thread ID will be stored upon successful creation.<br>
**attr**: An optional pointer to a pthread_attr_t structure containing thread attributes. If NULL is passed, default attributes are used.<br>
**start_routine**: A pointer to the function that the new thread will execute once it starts. This function should have the signature void *(*start_routine) (void *).<br>
**arg**: A pointer to the argument that will be passed to the start_routine function when the thread is created.<br><br>
Return Value:<br><br>
If the thread creation is successful, pthread_create returns 0.
If an error occurs, it returns a non-zero error code indicating the nature of the error.


>**int pthread_join(pthread_t thread, void **retval);**

Parameters:

**thread**: The thread ID of the thread to be joined.<br>
**retval**: A pointer to a location where the exit status of the joined thread will be stored. If NULL is passed, the exit status information is discarded.

Return Value:

If the thread join is successful, pthread_join returns 0.
If an error occurs, it returns a non-zero error code indicating the nature of the error.

>**void pthread_exit(void *retval);**

Parameters:

**retval**: A pointer to the exit status of the calling thread. This value will be available to any thread that joins the calling thread using pthread_join.

Return Value:

pthread_exit does not return a value. It terminates the calling thread.

## Let's see how we apply thread into the matrix multiplacation process

Here is the code for a function that calculates matrix multiplication. It takes matrix sizes as input *from the user, and its approximate complexity is
 
 >**O(matrix size^3)**
 
 . Let's optimize its execution time by using threads.

```cpp
int matrix_mul(int** a, int** b, int** c,int len){
    //mul of matrix
  for(int i=0;i<len;i++){
      for(int j = 0;j<len;j++){
         c[i][j] = 0;
         for(int k = 0;k<len;k++){
            c[i][j] += a[i][k] * b[k][j];
         }
      }
   }
   return 0;
}
```
Creating threads for the j-loop and k-loop in the above code is not practical, especially when the matrix size is 1000. If the matrix size is 1000, it would result in creating a million threads, which is not feasible due to the significant overhead associated with thread creation and management.

```cpp
#include <pthread.h>

type struct{
  int i;
  int** a;
  int** b;
  int** c;
  int len;
}matrix_thread_arg;

void *matrix_mul_thread_kernel(void* arg){

  matrix_thread_arg *parg = (matrix_thread_arg*)arg;
  int i = parg->i;
  int** a = parg->a;
  int** b = parg->b;
  int** c = parg->c;
  int len = parg->len;

  for(int j = 0;j<len;j++){
     c[i][j] = 0;
     for(int k = 0;k<len;k++){
        c[i][j] += a[i][k] * b[k][j];
     }
  }
}

int matrix_mul_thread(int** a, int** b, int** c, int len){
  pthread_t *a_thread;
  int i = 0;
  matrix_thread_arg arg;

  void *thread_result;

  a_thread = (pthread_t*)malloc(len*sizeof(pthread_t));

  //mul of matrix
  for(i=0;i<len;i++){
    arg.i = i;
    arg.a = a;
    arg.b = b;
    arg.c = c;
    arg.len = len;

    pthread_create(a_thread+i,NULL, *matrix_mul_thread_kernel,(void*)&arg);
  }

  for(i = 0;i<len;i++){
    pthread_join(a_thread[i],&thread_result);
  } 
  return 0;
}

```

Makefile needs to be modified too

```cpp
TARGET=main
OBJ=main.o matrix.o
CC=gcc
CFLAGS=-c -D_REENTRANT
LFLAGS=-pthread -lpthread

all: $(TARGET)

$(TARGET): $(OBJ)
  $(CC) -o $(TARGET) $(LFLAGS) $(OBJ)

.c.o:
  $(CC) $(CFLAGS) $<

clean:
  rm -rf *.o $(TARGET)
```

## Result

### Let's excuete

```cpp
ubuntu@ubuntu-vm:~/Work/matrix_example$ make
gcc -c -D_REENTRANT main.c
gcc -c -D_REENTRANT matrix.c
gcc -o main -pthread -lpthread main.o matrix.o
ubuntu@ubuntu-vm:~/Work/matrix_example$ ./main 1500
```


>processed time : ***11.834029*** (s)<br>processed time using thread: ***4.110313*** (s)


### Core status in thread

![core_without_thread](/assets/images/2023-12-12-thread/withoutthread.png)

The picture above shows single core is running since no thread is included

![core_with_thread](/assets/images/2023-12-12-thread/withthread.png)

As can be seen above, 6 cores are in parallel working for a million number of threads. If processing is not long enough to be threaded, overhead time made that no require using thread.  




